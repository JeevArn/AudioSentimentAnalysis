# AudioSentimentAnalysis
Projet de sentiment analysis sur le corpus audio RAVDESS
Article de r√©f√©rence : Singh, P. Nagrath, P. (2022). Vocal Analysis and Sentiment Discernment using AI. *Fusion: Practice and Applications*, (), 100-109. DOI: https://doi.org/10.54216/FPA.070204

## Auteurs
- AROUN Jeevya
- NGAUV Nicolas
- THEZENAS Anissa

## Usage
### 1. T√©l√©charger le dataset

Le dataset RAVDESS Speech et Song est disponible sur Kaggle :
```console
#!/bin/bash
curl -L -o ~/Downloads/ravdess-emotional-speech-audio.zip\
  https://www.kaggle.com/api/v1/datasets/download/uwrfkaggler/ravdess-emotional-speech-audio

curl -L -o ~/Downloads/ravdess-emotional-song-audio.zip\
  https://www.kaggle.com/api/v1/datasets/download/uwrfkaggler/ravdess-emotional-song-audio
```
**ATTENTION** Il faut obligatoirement supprimer les dossiers `audio_speech_actors_01-24/` et `audio_song_actors_01-24/` pr√©sents dans les dossiers t√©l√©charg√©s car ils contiennent des doublons et peuvent donc influencer les r√©sultats obtenus sur le test set.
Placer le dataset dans le dossier `data/`.

### 2. Lancer les scripts
```
python3 train_with_LSTM.py
python3 train_with_CNN_regularization_scheduler.py
python3 script_svm.py
```

## LSTM Jeevya üë©üèΩ‚Äçüíª
Nous avons cr√©√© un script python qui permet d'entrainer un mod√®le LSTM sur la reconnaissance de sentiments sur des extraits audio parl√©s et chant√©s provenant du corpus RAVDESS Speech & Song.
Comme dans le papier de r√©f√©rence, nous utilisons uniquement les labels 'sad', 'calm', 'happy', 'angry' et 'fearful' pour essayer d'avoir une meilleur accuracy en √©liminant les √©motions les plus difficile √† discerner.
On utilise un split train/test de respectivement 80% et 20% du corpus. L'entrainement se fait sur 70 epochs. Les hyperparam√®tres utilis√©s ne sont pas pr√©cis√©s dans l'article donc nous testons par nous m√™me des valeurs. Nous obtenons avec notre mod√®le **66% d'accuracy** sur le test set. Dans les prochaines √©tapes, nous comptons utiliser GridSearch pour essayer de trouver de meilleurs hyperparam√®tres et am√©liorer l'accuracy du mod√®le.

## CNN Nicolas üßëüèª‚Äçüíª

Conform√©ment √† l'article, on utilise un mod√®le de **r√©seau de neurones convolutifs** (ou r√©seau de neurones √† convolution) entrain√© sur une partie du dataset RAVDESS (80%) pour effectuer une classification des sentiments d√©tect√©s dans une autre partie du dataset RAVDESS (20%), afin d'√©valuer le mod√®le CNN.

Les CNN d√©tectent des motifs dans des matrices comme les **MFCC**, √† travers des couches convolutionnelles, d'activation et de pooling. Cela permet de construire des mod√®les capables de classifier les donn√©es avec efficacit√©.

Ici, on a essay√© de jouer avec la r√©gularisation (Dropout ou L2) et un apprentissage plus lent et progressif (avec un learning rate scheduler) pour essayer de stabiliser l'entra√Ænement, d'√©viter le sur-apprentissage et d'am√©liorer la capacit√© de g√©n√©ralisation du mod√®le.

### R√©sultats :

![CNN Training Validation Metrics](plots/CNN_training_validation_metrics.png)

**Training Accuracy** : 98,20%

**Validation Accuracy** : 77,13%

**Training Loss** : 0,3042

**Validation Loss** : 0,8233


![CNN Confusion Matrix](plots/CNN_confusion_matrix.png)

**Structure g√©n√©rale** :

Les lignes repr√©sentent les labels r√©els, et les colonnes repr√©sentent les labels pr√©dits.<br>
Les valeurs diagonales indiquent les pr√©dictions correctes, et les autres cellules montrent les erreurs de classification.

**Analyse par classe** :

Classe "angry" (angry ‚Üí ligne 1) :<br>
70 pr√©dictions correctes.<br>
17 pr√©dictions incorrectes : 10 instances class√©es comme "fearful" et 7 comme "happy".<br>
Interp√©tation et discussion : Le mod√®le semble confondre "angry" avec des √©motions comme "fearful" ou "happy", ce qui peut s'expliquer par des similitudes acoustiques entre des √©motions intenses comme la col√®re et le bonheur ou encore la peur (le corpus √©tant compos√© de voix parl√©es et chant√©es par des acteurs, et donc plus pr√©par√©es que spontan√©es). √Ä noter que la confusion entre "angry" et "fearful" provient de l'apport de l'apprentissage avec les voix chant√©es, car lors des tests avec les voix parl√©es seulement, nous n'avions pas ce r√©sultat.

Classe "calm" (calm ‚Üí ligne 2) :<br>
59 pr√©dictions correctes.<br>
11 pr√©dictions incorrectes : 8 instances class√©es comme "sad" et 3 comme "happy".<br>
Interp√©tation et discussion : Le mod√®le confond parfois le calme ("calm") avec des √©motions n√©gatives comme "sad". Cela est compr√©hensible, car une voix calme peut √™tre difficile √† distinguer d'une voix l√©g√®rement triste ou pos√©e dans le contexte audio (les spectres audio peuvent √™tre similaires). Pour les petites confusion entre "calm" et "happy", toutes les expressions de la joir ne sont pas forc√©ment associ√©es √† des hauteurs vocales √©lev√©es, et si les variations de pitch dans "happy" ne sont pas marqu√©es (par exemple, une expression de bonheur tranquille ou r√©serv√©), elles peuvent √™tre per√ßues comme similaires √† une voix calme.

Classe "fearful" (fearful ‚Üí ligne 3) :<br>
52 pr√©dictions correctes.<br>
17 pr√©dictions incorrects : 13 instances confondues avec "sad" et 4 avec "angry".<br>
Interp√©tation et discussion : Cela montre une confusion notable entre la peur ("fearful") et la tristesse ("sad"), souvent li√©e √† des signaux audio de faible intensit√© ou tonalit√© similaire : certaines caract√©ristiques acoustiques associ√©es √† la peur (comme un ton bas ou tremblant) peuvent ressembler √† celles d'une tristesse prononc√©e. Pour la confusion entre la peur ("fearful") et la col√®re ("angry"), elle vient certainement de l'apport des voix chant√©es (car lors de tests avec seulement les voix parl√©es, nous n'avions pas ce r√©sultat) : la prosodie lors du chant ou de la voix parl√©e est diff√©rente, et lorsque qu'on entraine et √©value le modl√®le sur un corpus compos√© de ces 2 types d'expression orale, il y a forc√©ment plus de confusion.

Classe "happy" (happy ‚Üí ligne 4) :<br>
58 pr√©dictions correctes.<br>
12 pr√©dictions incorrectes : 9 instances class√©es comme "fearful", 3 comme "angry" et 3 comme "sad".<br>
Interp√©tation et discussion : Les confusions avec "fearful" peuvent indiquer une confusion dans les fr√©quences audio plus aigu√´s. On peut aussi penser √† la variation rapide du pitch : les variations fr√©quentes ou abruptes du ton sont communes dans les √©motions comme la joie, la peur ou la col√®re. Par exemple, une voix joyeuse peut avoir des mont√©es rapides, tout comme une voix apeur√©e ou col√©rique. On peut aussi se dire que cela peut √™tre d√ª √† la diversit√© des expressions vocales du bonheur, qui peuvent parfois √™tre interpr√©t√©es comme des √©motions intenses (ce que sont la peur et la col√®re) ou plus m√©lancoliques (ce qui peut expliquer la confusion avec "sad").

Classe "sad" (sad ‚Üí ligne 5) :<br>
51 pr√©dictions correctes.<br>
16 pr√©dictions incorrectes : 9 instances confondues avec "calm", 7 avec "fearful" et 7 avec "happy".<br>
Interp√©tation et discussion : "Sad" est globalement bien reconnue, mais il existe des confusions significatives avec "Calm". Cette confusion est fr√©quente dans les mod√®les de classification audio, car des √©motions comme la tristesse et le calme partagent souvent des tonalit√©s douces et des rythmes lents. Pour "sad" et "fearful", ces deux √©motions sont souvent exprim√©es avec des tonalit√©s graves ou basses dans la voix, ce qui peut expliquer la confusion. Les confusions entre "sad" et "happy" peuvent sembler contre-intuitives √©tant donn√© que ces deux √©motions ont une valence √©motionnelle oppos√©e (n√©gative pour "sad", positive pour "happy")... Cependant, elles peuvent survenir dans un mod√®le de classification audio √† cause de l'overlap qu'il peut parfois y avoir dans les spectres acoustiques : les descripteurs acoustiques utilis√©s par les mod√®les, comme les MFCC, se concentrent principalement sur les propri√©t√©s spectrales (fr√©quence et √©nergie). Ils peuvent ne pas capturer les diff√©rences subtiles de valence √©motionnelle.



## SVM (Support Vector Machine) Anissaüë©üèæ‚Äçüíª

### Objectif

Le mod√®le SVM est utilis√© pour effectuer une classification des √©motions d√©tect√©es dans le corpus audio. SVM est un algorithme d‚Äôapprentissage supervis√© qui s√©pare les diff√©rentes classes en maximisant la marge entre celles-ci gr√¢ce √† un hyperplan optimal.

### Rapport de classification :

| **Classe** | **Pr√©cision** | **Rappel** | **F1-Score** | **Support** |
|------------|--------------:|-----------:|-------------:|------------:|
| 0          | 0.66          | 0.81       | 0.72         | 31          |
| 1          | 0.81          | 0.74       | 0.77         | 34          |
| 2          | 0.80          | 0.67       | 0.73         | 42          |
| 3          | 0.64          | 0.57       | 0.61         | 40          |
| 4          | 0.62          | 0.71       | 0.66         | 45          |

### Moyenne et Accuracy :

| **M√©trique**      | **Valeur** |
|-------------------:|----------:|
| **Accuracy**      | 0.69      |
| **Macro Average** | 0.70      |
| **Weighted Avg**  | 0.70      |

